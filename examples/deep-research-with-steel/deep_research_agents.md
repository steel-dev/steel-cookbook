Building an Open-Source Deep Research Agent: Core Agentic Loop Explained

Deep Research refers to an AI-driven research workflow that autonomously finds, organizes, and synthesizes information from the web to answer complex queries with comprehensive, well-cited reports ￼. OpenAI’s Deep Research (2025) and Google Gemini’s Deep Research feature both exemplify this approach, going beyond simple search by having an agentic LLM actively browse the web, reason through multi-hop questions, and compile findings with sources ￼ ￼. To replicate these capabilities in open-source, it’s crucial to understand the core agentic loop at the heart of Deep Research. This loop comprises a sequence of planning, tool-using, and self-refining steps that enable the AI agent to iteratively gather information and produce a detailed answer. Below, we first outline the core loop in simple terms, then dive deeper into each component and technical details for an open-source implementation.

Core Agentic Loop Overview

At a high level, the Deep Research agent operates in a loop of Plan → Search → Evaluate → Refine → Synthesize. In practice, this means the agent will:
	•	Plan: Break down the user’s complex question into sub-questions or search queries (an initial research plan).
	•	Search: Execute web searches (or use other tools) to retrieve information relevant to those queries.
	•	Evaluate & Refine: Analyze the gathered info to see if the question is fully answered; if not, identify knowledge gaps and formulate new queries or steps, looping back into search. This self-reflection continues until sufficient information is collected to address all aspects of the query.
	•	Synthesize Answer: Finally, compile the collected knowledge into a coherent, structured report or answer, complete with citations and any additional enhancements (charts, summaries, etc.).

In essence, the agent starts with an initial set of queries and continues searching until it has enough information to answer the question ￼. Throughout this process, it autonomously uses tools (like web browsers, search APIs, or even code execution) and organizes its actions in a series of steps guided by an internal logic or “agentic framework” ￼. The result is a thorough answer that saves a human many hours of manual research, as noted by Google: Gemini’s Deep Research can spend a few minutes to produce a report that might otherwise take hours of digging through links ￼ ￼.

Step 1: Planning and Decomposition of the Query

The loop begins with planning. Given a complex query, the agent’s first task is to decompose the problem into smaller sub-tasks or queries. This often involves identifying the key aspects or sub-questions that must be answered to address the overall question ￼. For example, if the user asks: “What are the five largest renewable energy companies and their current stock prices?”, the agent recognizes this requires two phases – finding the top companies, then finding each stock price ￼. Similarly, for a question about, say, the impact of new steel tariffs on global markets, an agent might plan to first gather data on the tariff details, then find analyses or economic reports on their impact.

In OpenAI’s Deep Research API, this planning is built-in: the agent “autonomously plans sub-questions” based on the user query ￼. Google’s Gemini explicitly shows this step – upon receiving the question, Gemini creates a multi-step research plan that the user can revise or approve before proceeding ￼. The plan is essentially a list of search queries or topics to investigate, ensuring the agent covers all facets of the question.

Technically, implementing this in open source can be done by prompting a strong reasoning LLM (the “Planner”) to output a list of relevant search queries or a structured plan. Together AI’s open Deep Research uses a dedicated planner model (e.g. Qwen-72B) to generate an initial set of important queries from the user’s question ￼. This planning component is critical: it sets the direction for the research and ensures the agent doesn’t approach the task blindly.

Step 2: Autonomous Web Search and Retrieval

After planning, the agent enters the search and retrieval phase. Here, the agent uses a web browsing tool to execute the planned queries and gather information. The agent effectively behaves like a researcher using a search engine: performing a search, clicking results, and reading content. In OpenAI’s workflow, the agent leverages a web_search_preview tool under the hood to fetch web pages relevant to each sub-question ￼ ￼. Google’s Gemini Deep Research similarly “browses the web the way you do: searching, finding interesting pieces of information…” in an automated loop ￼.

For an open-source agent, this means integrating a search API or browser automation. Some implementations use simple search engine APIs (Bing, Google Custom Search, etc.) or dedicated libraries like Tavily (used by Together’s version) which can fetch raw webpage content in one go ￼. Others have built or used open browser tools; for instance, Hugging Face’s prototype started with a “simple text-based web browser” tool adapted from Microsoft’s Magentic-One agent ￼. Another emerging tool is Steel (steel.dev), an open-source headless browser API for agents, which could be used to navigate pages programmatically. Regardless of the specific tool, the agent’s loop involves calling the search tool with a query, retrieving the top results, and potentially scraping the page content.

Each retrieved source is stored for analysis. In code, this might involve the agent saving the text of articles or search snippets as variables or files. (Notably, Hugging Face’s CodeAgent approach makes this convenient – since the agent writes code, it can assign retrieved text to variables for later use ￼.) The key is that the agent now has a growing collection of information related to the question.

Tool Implementation Note: Designing the agent’s tool interface is a crucial part of the loop. OpenAI’s system has an internal agentic framework managing these search actions ￼, and Google built a new agentic system leveraging Google’s search expertise to guide Gemini’s browsing ￼. In open source, developers have used frameworks like LangChain or smolagents to handle tool usage. Hugging Face’s open DeepResearch agent opted for a code-native agent where the LLM outputs Python code that calls search functions (instead of a JSON schema of actions). This proved advantageous: writing actions in code was ~30% more concise (fewer steps/tokens) than JSON-based plans, improving efficiency and cost ￼. Code-based agents also handle state better; e.g. an image or text fetched in one step can be stored in a variable and reused later, which is harder to do reliably with JSON step outputs ￼.

Step 3: Self-Reflection and Iterative Research Loop

Once the agent has some initial sources, it performs a self-reflection or evaluation step. This is where the agent asks: “Given the information I’ve gathered so far, have all parts of the question been answered? What’s missing?” Any identified knowledge gaps will trigger refinement – essentially looping back to planning new queries or adjusting the search.

Together’s Open Deep Research highlights this as a core component: after initial searches, they “ask an LLM to evaluate whether any knowledge gaps remain unfilled by the current sources”, drawing on Self-Reflection techniques ￼. If gaps are found, the agent generates additional queries to fill those gaps, and the Plan→Search→Evaluate cycle repeats. This loop continues “until it has collected enough information to answer the question” fully ￼.

Google’s description of Gemini’s Deep Research vividly illustrates this iterative loop. Over a few minutes, Gemini continuously refines its analysis, searching, reading info, then launching new searches based on what it learned, repeating this process multiple times ￼. For instance, if our agent researching steel tariffs finds data on current tariff rates but not their economic impact, it might spawn a new query like “economic impact of 2023 steel tariffs study” and continue digging.

In an open-source agent, implementing reflection can be done by prompting an LLM to analyze the gathered notes/summaries and explicitly ask it: “Are there aspects of the user’s question not yet answered by the above information? If so, list what to research next.” The agent can then turn those suggestions into new search actions. This approach is inspired by academic work on reflective agents (e.g. Shinn et al. 2023 on self-reflection for AI reasoning ￼). It helps prevent the agent from stopping too early or missing subtopics.

It’s important to put a cap on iterations for practicality – e.g. limiting to a few cycles or a time budget – since an agent could theoretically loop endlessly if not careful. In practice, 2–3 rounds often suffice for well-scoped questions ￼, but more may be needed for highly complex tasks (the open-source Together implementation allows multiple reflection loops, which is why a full run can take a few minutes) ￼.

Throughout this iterative searching, caching can be very useful. Open-source projects noted that running many web queries can become expensive or slow, so they employed caching of results ￼. For example, if the agent revisits a query or the same page, it can reuse the stored content instead of fetching again. (One must use cache invalidation or TTL to avoid stale info in long-term usage ￼, but for short-term loops it’s a big efficiency gain.)

Step 4: Summarizing and Managing Retrieved Content

As the agent accumulates many sources and pages of text, a practical challenge arises: how to manage a large volume of content within the LLM’s context limit. Deep Research agents often end up with dozens of pages of text, which cannot all be fed into the final answer-writing step directly. To address this, open-source implementations introduce a summarization stage in the pipeline.

The idea is to condense each retrieved source into a concise summary or extract the key facts, so that only the most relevant information is carried forward. Together’s pipeline explicitly does this: they use an LLM to summarize the raw content of each source, keeping only the important information ￼. For example, if one source is a 5,000-word PDF on steel industry statistics, the summarizer might produce a 200-word summary of its crucial points. This obviously risks losing some context or nuance (and sometimes the summarizer might omit details needed for citing, etc.), but it dramatically reduces token usage and fits more knowledge into the context window ￼.

An alternative or complementary approach is to perform information extraction. Together’s architecture even includes a specialized “JSON extractor” model tasked with pulling structured facts from text ￼. This can ensure that, say, specific figures or names from sources are captured in a machine-readable form, which can then be easily incorporated into the final report or used to generate charts.

By summarizing or extracting, the agent transforms a large set of documents into a more digestible set of notes. This intermediate knowledge base is what the final answer composer will use. It’s worth noting that if using a single monolithic agent (like in a ReAct loop), summarization might be done on the fly (“summarize tool output before storing to memory”). In multi-component pipelines (like Together’s), it’s a distinct step with its own model, chosen for efficiency on long inputs (e.g. they use a Llama 70B model fine-tuned for summarization) ￼.

Step 5: Ranking Sources and Ensuring Quality

Before synthesizing the answer, one more curation step can be beneficial: ranking or filtering the gathered sources by relevance and quality. The goal is to have the final answer lean on the most trustworthy, relevant references. Together’s workflow incorporates this by using an LLM to rank all sources by relevance and then cite primarily the high-quality ones ￼. This is akin to a re-ranking step common in information retrieval pipelines – essentially scoring which summaries or articles best address the query.

The agent might drop sources that are redundant or low-quality, and focus on, say, the top 5–7 sources for writing the report. This improves the clarity and factual accuracy of the final output ￼. It also helps mitigate hallucination, because the model is guided to use actual content from those top sources when formulating answers.

In open-source agents, such ranking could be as simple as using a GPT-4 or similar model to read all the summaries and list which ones seem most on-point for the question. Some implementations might use vector embeddings or information retrieval scoring for this, but a direct LLM judgment can work since it can understand the question and the content semantically.

By the end of this stage, the agent has a curated set of concise source summaries or facts, each linked to an original source URL, ready to be woven into the final answer. All the heavy lifting in terms of research is done; what remains is to present the information effectively.

Step 6: Synthesizing the Final Report (Writing with Citations)

The final step in the core loop is writing the comprehensive answer or report. Here, the agent (usually a strong LLM specialized in writing) takes the consolidated information from prior steps and generates a structured, coherent response that directly addresses the user’s query. Crucially, it will cite sources inline to back up facts and give the user references ￼ ￼.

Both OpenAI and Gemini emphasize that the output is structured and citation-rich. The OpenAI Deep Research API returns a structured report with inline citations and source metadata ￼ ￼. Google’s Gemini output similarly is an easy-to-read report with links to original sources for each key finding ￼. In practice, these citations might appear as superscript numbers or reference tags (like the ones you see in this answer), which correspond to a bibliography of source URLs.

To achieve this in open source, the final writing prompt typically includes all the top source summaries (or extracted facts) and instructs the model to “Write a detailed answer to the user’s question, using the provided information. Cite the sources for each claim or statistic.” The model must then map pieces of its generated text to the original sources. This can be done by having it output special tokens or markers for each source (as OpenAI’s API does with annotation objects ￼), or by formatting references as e.g. [Source 1] in the text which are later replaced with actual links.

The quality of the final model matters a lot here. Together AI’s project uses a very powerful LLM (their DeepSeek-V3 model) as the report writer to ensure the answer is well-written and accurate ￼. They found that using a top-tier model for writing improves output quality, though at higher cost ￼ ￼. In an open setting, one might choose an open-source LLM like Llama-2 or an instruction-tuned variant for this step, balancing quality and compute constraints.

The output at this stage is the end-user-facing result: a long-form answer that reads like an analyst’s report. It typically includes multiple sections or paragraphs covering different angles of the query, and it may present data in a friendly way (bullet points, short sections for each sub-question, etc.) to enhance readability ￼.

One of the design goals is communication – not just dumping facts, but making the report easy to understand ￼. For example, the agent might create a brief introduction summarizing the findings, then detailed sections, and possibly a conclusion. All along, citations are embedded so the user can verify claims. This addresses a key trust issue: by always citing sources, the agent’s output remains transparent and the user can fact-check any part of the answer ￼.

Finally, the answer may include some post-processing enhancements. Together’s implementation adds a few bells and whistles at this stage: it can generate a Mermaid.js diagram or chart if the data suggests one ￼ ￼, and even produce a short podcast audio summary of the report using a text-to-speech model ￼. These are optional add-ons beyond the core loop, but they illustrate how the final output can be enriched. For instance, an agent might notice the data could form a timeline or pie chart and include a generated chart in the report (the LLM can output the code for a chart which is then rendered externally ￼). Such features make the report more engaging, though they require additional tool integrations (JavaScript rendering, TTS models, etc.).

Agent Architecture and Technical Design Considerations

To build an open-source Deep Research agent, one needs to orchestrate the above steps, possibly using multiple specialized components or a single multi-tool agent. There are two primary architectural patterns seen in the community:
	•	Pipeline of Specialized Models (Modular): Different LLMs or modules handle different stages. As seen in Together’s Open Deep Research, there are distinct roles: a planner LLM for the plan, a search tool for retrieval, a summarizer LLM for condensing content, an extractor for structuring data, and a report writer LLM for the final output ￼. Each component can be optimized for its task (for example, a smaller, faster model to summarize long texts, and a larger model for the final write-up). This modular approach can improve efficiency and let developers swap in better models for each part over time. The downside is the added complexity of managing data flow between models and ensuring consistency (e.g., the planner and writer need to have a shared understanding of format).
	•	Single LLM Agent with Tools (Monolithic): A single agent loop where one LLM iteratively plans, calls tools, and decides when to stop and answer. This is typically implemented via a prompting framework (like ReAct prompt or code agent). Hugging Face’s 24-hour prototype followed this pattern using the smolagents library: the LLM was prompted to output code that uses a search() function, a read() function, etc., in a loop until it decided to print(answer) ￼ ￼. In this design, the LLM maintains the chain-of-thought internally. It’s conceptually simpler (one brain coordinating everything) and avoids interface friction between models. However, it often requires a very capable model with a large context (to remember all it found) and careful prompt engineering to keep the agent on track. Debugging can also be tricky since the logic lives in the LLM’s “mind” rather than explicit code.

Open-source efforts have explored both patterns. For instance, the community reproductions cited by Hugging Face include implementations with different libraries and approaches ￼. Some used LangChain with a single LLM + tools, others used a vector database for storing snippets, etc. There is an active ecosystem of such agents (projects by users dzhng, nickscamara, Jina AI’s version, Mendable’s Firecrawl, etc., all approached the same goal with slight variations ￼). This shows there’s no one “right” way – the core loop remains the same, but you can distribute it across components or not.

Tooling and Extensions: In building an open agent, one should also consider what tools beyond web search might be useful. OpenAI’s Deep Research API, for example, allows a code interpreter tool as well ￼. This means the agent could, say, run Python to crunch numbers or parse data tables it finds. For a “steel cookbook” scenario, if the agent finds a CSV of steel production figures, a code tool could help analyze it. While not strictly necessary for core operation, such tools extend the agent’s capabilities. In open source, you could integrate a Python REPL, a calculator, or even custom domain tools similarly, as long as the agent knows when to invoke them.

Browser Automation: A notable aspect is web interaction depth. Current open implementations mostly fetch static page content (text) for the LLM to read. OpenAI’s private system reportedly has a more advanced browser controller (codenamed Operator) that can scroll pages, click buttons, etc. ￼. Fully open-source agents might eventually incorporate headless browsers or GUI automation to handle dynamic content or login-required sites. This is an area for future improvement, as one community roadmap noted the need to move “beyond text-only web interaction” to reach parity with OpenAI’s agent ￼. Tools like the aforementioned Steel browser API, or even hooking into a real browser with something like Playwright, could be used to give the agent richer web navigation abilities.

Performance and Costs: Using these agents comes with non-trivial compute costs, due to multiple LLM calls and searches. The design should consider latency and expense. For example, Together’s team balanced quality vs cost by assigning different model sizes to different roles and leveraging caching of search results ￼ ￼. They report a typical Deep Research response taking 2–5 minutes to generate on their setup ￼. For faster turnaround, one might use a smaller model for the writer or limit the number of iterations. There’s always a trade-off between depth of research and speed.

In summary, the core agentic loop of Deep Research involves iterative planning, tool use, and reflection leading to a final well-referenced report. Implementing this loop in open source requires stitching together LLM reasoning with search and other tools in a robust way. Thanks to community efforts, we have examples of how to do this, from Hugging Face’s code-oriented agent (built in 24 hours) to Together AI’s fully-fledged pipeline. All follow the same fundamental principles outlined above. By carefully designing each component – query planning, searching, reflecting, summarizing, and writing – and ensuring they feed into each other, one can build an open-source agent that mimics the capabilities of OpenAI’s and Gemini’s Deep Research, effectively turning raw web data into actionable knowledge ￼ ￼.

Sources: The information above was synthesized from several resources. Together AI’s blog post on Open Deep Research provided insights into the architecture and design choices of a multi-LLM agent ￼ ￼. The Hugging Face blog on reproducing DeepResearch in 24h contributed details on agent frameworks, code-based actions, and community implementations ￼ ￼. OpenAI’s Cookbook documentation on the Deep Research API illustrated how the closed-source version plans and uses tools ￼ ￼. Finally, Google’s announcement of Gemini’s Deep Research feature confirmed the iterative search loop and user-facing behavior of such agents ￼ ￼. All these sources converge on the concept of an agentic loop that transforms a complex question into a thoughtful answer by planning, searching, reasoning, and refining in cycles, which is the essence of Deep Research.